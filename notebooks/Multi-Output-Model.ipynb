{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Output Model to Predict Macronutrient Profile\n",
    "\n",
    "In this notebook we are going to improve on the current model to predict calores, and instead we will predict all 3 macronutrients. Here is how it will work:\n",
    "\n",
    "- The user will input a recipe name.\n",
    "- Then a list of suggested ingredients will appear, and the user can optionally edit this by either adding/subtracting ingredients or modifying quantities\n",
    "    - This will be obtained for now by just running the cosine similarity with the recipe names in the data, and taking the ingredients from the top 5 or so results.\n",
    "    - Quantities may be ignored if model can't handle numeric inputs, but LLMs can if going that route.\n",
    "    - Being able to edit this could be a paid feature, i.e. changing quantities or adding/subtracting ingredients.\n",
    "- The model will then take as input, the recipe name concatenated with the ingredients list. Perhaps with quantities as well if using an LLM.\n",
    "    - Consider adding dietery type/preference here as well.\n",
    "- Finally, a multi-output regression model will be used to predict the macronutrients, and we can take ratios by dividing each macro by the total calorie count. The aim here is that during training, the model will learn to optimize its parameters to minimize the error between the predicted and ground truth ratios derived from the normalized macronutrient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "#keeping all imports at the top\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "for module_name in ['pandas',]:\n",
    "    module = __import__(module_name)\n",
    "    print(f\"{module_name}: {module.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uri', 'label', 'image', 'source', 'url', 'shareAs', 'yield',\n",
       "       'dietLabels', 'healthLabels', 'cautions', 'ingredientLines',\n",
       "       'ingredients', 'calories', 'totalWeight', 'totalTime', 'cuisineType',\n",
       "       'mealType', 'dishType', 'totalNutrients', 'totalDaily', 'digest',\n",
       "       'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../recipes.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['1 pound green beans, trimmed', '1 tablespoon...\n",
       "1        ['1 1/2 lb green beans, stem ends trimmed', '1...\n",
       "2        ['1 stick (8 tbsp.) unsalted, cultured butter'...\n",
       "3        ['2 teaspoons walnut oil', '1 pound green bean...\n",
       "4        ['1 pound green beans, trimmed', '2 teaspoons ...\n",
       "                               ...                        \n",
       "13267    ['* 2tablespoons olive oil', '* 1 large red be...\n",
       "13268    ['Two 6-ounce cans white meat tuna packed in w...\n",
       "13269    ['16 ounces low-sodium chunk light tuna, drain...\n",
       "13270    ['1 can (3 ounces) tuna, drained', '1 tablespo...\n",
       "13271    ['1 can (3 ounces) chunk light tuna in water, ...\n",
       "Name: ingredientLines, Length: 13272, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ingredientLines']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ingredientLines column is all we need if we concatenate ingredients with the recipe name, but since we want to adjust the ingredients and quantity for the input, we will need to get the information from the ingredients column manually instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_col = df['ingredients'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '1 pound green beans, trimmed',\n",
       "  'quantity': 1.0,\n",
       "  'measure': 'pound',\n",
       "  'food': 'green beans',\n",
       "  'weight': 453.59237,\n",
       "  'foodCategory': 'vegetables',\n",
       "  'foodId': 'food_aceucvpau4a8v6atkx5eabxyoqdn',\n",
       "  'image': 'https://www.edamam.com/food-img/891/89135f10639878a2360e6a33c9af3d91.jpg'},\n",
       " {'text': '1 tablespoon butter, (optional)',\n",
       "  'quantity': 1.0,\n",
       "  'measure': 'tablespoon',\n",
       "  'food': 'butter',\n",
       "  'weight': 14.2,\n",
       "  'foodCategory': 'Dairy',\n",
       "  'foodId': 'food_awz3iefajbk1fwahq9logahmgltj',\n",
       "  'image': 'https://www.edamam.com/food-img/713/71397239b670d88c04faa8d05035cab4.jpg'},\n",
       " {'text': 'Coarse salt and ground pepper',\n",
       "  'quantity': 0.0,\n",
       "  'measure': None,\n",
       "  'food': 'Coarse salt',\n",
       "  'weight': 2.80675422,\n",
       "  'foodCategory': 'Condiments and sauces',\n",
       "  'foodId': 'food_a1vgrj1bs8rd1majvmd9ubz8ttkg',\n",
       "  'image': 'https://www.edamam.com/food-img/694/6943ea510918c6025795e8dc6e6eaaeb.jpg'},\n",
       " {'text': 'Coarse salt and ground pepper',\n",
       "  'quantity': 0.0,\n",
       "  'measure': None,\n",
       "  'food': 'ground pepper',\n",
       "  'weight': 1.40337711,\n",
       "  'foodCategory': 'Condiments and sauces',\n",
       "  'foodId': 'food_b6ywzluaaxv02wad7s1r9ag4py89',\n",
       "  'image': 'https://www.edamam.com/food-img/c6e/c6e5c3bd8d3bc15175d9766971a4d1b2.jpg'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_col[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredient_aspect(row, aspect):\n",
    "    lst = []\n",
    "    for j in range(len(row)):\n",
    "        lst.append(row[j][aspect])\n",
    "    return lst\n",
    "\n",
    "food_ingredients = ingredients_col.apply(lambda row: get_ingredient_aspect(row, 'food')).rename('foodItem')\n",
    "quantity_ingredients = ingredients_col.apply(lambda row: get_ingredient_aspect(row, 'quantity')).rename('quantity')\n",
    "measure_ingredients = ingredients_col.apply(lambda row: get_ingredient_aspect(row, 'measure')).rename('measurementUnit') # will need this to understand quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `healthLabel` is the dietary type/preference like vegan, pescaterian, etc. We will only select few options though as it is a multilabel column, and the user can only select 1 for now, from: ['Mediterranean', 'Vegetarian', 'Vegan', 'Red-Meat-Free', 'Paleo', 'Pescatarian']. When reducing the healthLabels column from multilabel to categorical, we need to define a priority order, and if none of these are there then the dish is balanced, so we will add that as an option. In the future, some analysis on this column should be done to improve the priority order, rather than relying on domain knowledge. Alternatively, and option to select multiple could be implemented instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_labels = df['healthLabels'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all unique values of health labels. If we had more data it might be worth it to just keep all of these health labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sesame-Free', 'Tree-Nut-Free', 'Soy-Free', 'Kidney-Friendly', 'Egg-Free', 'Mollusk-Free', 'Low Sugar', 'Alcohol-Cocktail', 'Vegan', 'Paleo', 'Gluten-Free', 'Peanut-Free', 'Crustacean-Free', 'Keto-Friendly', 'Dairy-Free', 'Shellfish-Free', 'Lupine-Free', 'Sugar-Conscious', 'Kosher', 'Immuno-Supportive', 'Wheat-Free', 'Sulfite-Free', 'Mustard-Free', 'Red-Meat-Free', 'Pork-Free', 'Vegetarian', 'Fish-Free', 'Low Potassium', 'DASH', 'FODMAP-Free', 'Celery-Free', 'Pescatarian', 'Mediterranean', 'Alcohol-Free', 'No oil added'}\n"
     ]
    }
   ],
   "source": [
    "unique_health_labels = []\n",
    "for lst in health_labels:\n",
    "    for health in lst:\n",
    "        unique_health_labels.append(health)\n",
    "\n",
    "print(set(unique_health_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_order = ['Vegan', 'Vegetarian', 'Pescatarian', 'Paleo', 'Red-Meat-Free', 'Mediterranean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_priority(labels):\n",
    "    for label in priority_order:\n",
    "        if label in labels:\n",
    "            return label\n",
    "    return 'Balanced'  # Handle case where no label matches priority_order, in which case the diet is balanced\n",
    "\n",
    "# Apply function to the multilabels series\n",
    "priority_health_labels = health_labels.apply(replace_with_priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    "\n",
    "Now let's get the macros and calories can be calcualted from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uri', 'label', 'image', 'source', 'url', 'shareAs', 'yield',\n",
       "       'dietLabels', 'healthLabels', 'cautions', 'ingredientLines',\n",
       "       'ingredients', 'calories', 'totalWeight', 'totalTime', 'cuisineType',\n",
       "       'mealType', 'dishType', 'totalNutrients', 'totalDaily', 'digest',\n",
       "       'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = df['totalNutrients'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ENERC_KCAL', 'FAT', 'FASAT', 'FATRN', 'FAMS', 'FAPU', 'CHOCDF', 'CHOCDF.net', 'FIBTG', 'SUGAR', 'PROCNT', 'CHOLE', 'NA', 'CA', 'MG', 'K', 'FE', 'ZN', 'P', 'VITA_RAE', 'VITC', 'THIA', 'RIBF', 'NIA', 'VITB6A', 'FOLDFE', 'FOLFD', 'FOLAC', 'VITB12', 'VITD', 'TOCPHA', 'VITK1', 'WATER'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrients[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Energy', 'quantity': 331.96545205, 'unit': 'kcal'}\n",
      "{'label': 'Fat', 'quantity': 15.008954821, 'unit': 'g'}\n",
      "{'label': 'Saturated', 'quantity': 2.2059442775, 'unit': 'g'}\n",
      "{'label': 'Trans', 'quantity': 0.0, 'unit': 'g'}\n",
      "{'label': 'Monounsaturated', 'quantity': 9.9235888555, 'unit': 'g'}\n",
      "{'label': 'Polyunsaturated', 'quantity': 2.19255406715, 'unit': 'g'}\n",
      "{'label': 'Carbs', 'quantity': 47.8064322835, 'unit': 'g'}\n",
      "{'label': 'Carbohydrates (net)', 'quantity': 29.2874412985, 'unit': 'g'}\n",
      "{'label': 'Fiber', 'quantity': 18.518990985000002, 'unit': 'g'}\n",
      "{'label': 'Sugars', 'quantity': 22.359966893000003, 'unit': 'g'}\n",
      "{'label': 'Protein', 'quantity': 12.551760556500001, 'unit': 'g'}\n",
      "{'label': 'Cholesterol', 'quantity': 0.0, 'unit': 'mg'}\n"
     ]
    }
   ],
   "source": [
    "for nutrient in nutrients[1].keys():\n",
    "    print(nutrients[1][nutrient])\n",
    "\n",
    "    #just want to look at these more closely, but we don't need to look at the micronutrients\n",
    "    if nutrients[0][nutrient]['label'] == 'Cholesterol':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calories is fat x 9 + protein x 4 + carbs x 4 + fiber x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339.47538277900003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fat = 2.2059442775 + 9.9235888555 + 2.19255406715 #adding up all the different types of fat doesn't result in the total fat for some reason\n",
    "(15.008954821*9 + 29.2874412985*4 + 12.551760556500001*4) + 18.518990985000002*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.8064322835"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net carbs + fiber\n",
    "18.518990985000002 + 29.2874412985"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason when you add this up the calories isn't an exact match with the recorded calories in the calories column (which is the same as the Energy label here). And carbs is just net carbs + fiber, and since fiber is insoluble we won't be counting/predicting it. Also the discrepency doesn't seem to be just from counting or not counting Fiber. Thus, we will just use the calculation for now instead, counting only net carbs, protein, and fat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macros(nutrients_row):\n",
    "    macros_dct = {}\n",
    "\n",
    "    for nutrient in nutrients_row.keys():\n",
    "        if nutrients_row[nutrient]['label'] == 'Fat':\n",
    "            macros_dct['fat'] = nutrients_row[nutrient]['quantity']\n",
    "        elif nutrients_row[nutrient]['label'] == 'Protein':\n",
    "            macros_dct['protein'] = nutrients_row[nutrient]['quantity']\n",
    "        elif nutrients_row[nutrient]['label'] == 'Carbohydrates (net)':\n",
    "            macros_dct['carbs'] = nutrients_row[nutrient]['quantity']\n",
    "\n",
    "    return macros_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.559853</td>\n",
       "      <td>19.920021</td>\n",
       "      <td>8.567392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.008955</td>\n",
       "      <td>29.287441</td>\n",
       "      <td>12.551761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93.626455</td>\n",
       "      <td>29.120751</td>\n",
       "      <td>13.416711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fat      carbs    protein\n",
       "0  12.559853  19.920021   8.567392\n",
       "1  15.008955  29.287441  12.551761\n",
       "2  93.626455  29.120751  13.416711"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macros_df = pd.DataFrame(list(nutrients.apply(lambda row: get_macros(row))))\n",
    "\n",
    "#macros_df['calories'] = 9*macros_df['fat'] + 4*macros_df['protein'] + 4*macros_df['carbs']\n",
    "macros_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>healthLabels</th>\n",
       "      <th>recipeName</th>\n",
       "      <th>foodItem</th>\n",
       "      <th>quantity</th>\n",
       "      <th>measurementUnit</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Green Beans</td>\n",
       "      <td>[green beans, butter, Coarse salt, ground pepper]</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[pound, tablespoon, None, None]</td>\n",
       "      <td>12.559853</td>\n",
       "      <td>19.920021</td>\n",
       "      <td>8.567392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sauteed Green Beans</td>\n",
       "      <td>[green beans, olive oil, green-bean]</td>\n",
       "      <td>[1.5, 1.0, 1.0]</td>\n",
       "      <td>[pound, tablespoon, &lt;unit&gt;]</td>\n",
       "      <td>15.008955</td>\n",
       "      <td>29.287441</td>\n",
       "      <td>12.551761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Caramelized Green Beans</td>\n",
       "      <td>[butter, green beans]</td>\n",
       "      <td>[8.0, 1.5]</td>\n",
       "      <td>[tablespoon, pound]</td>\n",
       "      <td>93.626455</td>\n",
       "      <td>29.120751</td>\n",
       "      <td>13.416711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sautéed Fresh Green Beans</td>\n",
       "      <td>[walnut oil, green beans]</td>\n",
       "      <td>[2.0, 1.0]</td>\n",
       "      <td>[teaspoon, pound]</td>\n",
       "      <td>9.997903</td>\n",
       "      <td>19.368394</td>\n",
       "      <td>8.300740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Fancy Green Beans</td>\n",
       "      <td>[green beans, vegan margarine, sesame seeds, S...</td>\n",
       "      <td>[1.0, 2.0, 2.0, 0.0, 0.0]</td>\n",
       "      <td>[pound, teaspoon, teaspoon, None, None]</td>\n",
       "      <td>11.359097</td>\n",
       "      <td>20.608817</td>\n",
       "      <td>9.509045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  healthLabels                 recipeName  \\\n",
       "0   Vegetarian                Green Beans   \n",
       "1        Vegan        Sauteed Green Beans   \n",
       "2   Vegetarian    Caramelized Green Beans   \n",
       "3        Vegan  Sautéed Fresh Green Beans   \n",
       "4        Vegan          Fancy Green Beans   \n",
       "\n",
       "                                            foodItem  \\\n",
       "0  [green beans, butter, Coarse salt, ground pepper]   \n",
       "1               [green beans, olive oil, green-bean]   \n",
       "2                              [butter, green beans]   \n",
       "3                          [walnut oil, green beans]   \n",
       "4  [green beans, vegan margarine, sesame seeds, S...   \n",
       "\n",
       "                    quantity                          measurementUnit  \\\n",
       "0       [1.0, 1.0, 0.0, 0.0]          [pound, tablespoon, None, None]   \n",
       "1            [1.5, 1.0, 1.0]              [pound, tablespoon, <unit>]   \n",
       "2                 [8.0, 1.5]                      [tablespoon, pound]   \n",
       "3                 [2.0, 1.0]                        [teaspoon, pound]   \n",
       "4  [1.0, 2.0, 2.0, 0.0, 0.0]  [pound, teaspoon, teaspoon, None, None]   \n",
       "\n",
       "         fat      carbs    protein  \n",
       "0  12.559853  19.920021   8.567392  \n",
       "1  15.008955  29.287441  12.551761  \n",
       "2  93.626455  29.120751  13.416711  \n",
       "3   9.997903  19.368394   8.300740  \n",
       "4  11.359097  20.608817   9.509045  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_name = df['label'].rename('recipeName')\n",
    "\n",
    "relevant_cols_df = pd.concat([priority_health_labels, recipe_name, food_ingredients, quantity_ingredients, measure_ingredients, macros_df], axis=1)\n",
    "relevant_cols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 pound green beans\n",
      "1.0 tablespoon olive oil\n",
      "1.0 <unit> green-bean\n"
     ]
    }
   ],
   "source": [
    "ex_row = relevant_cols_df.iloc[1]\n",
    "\n",
    "for i in range(len(ex_row['foodItem'])):\n",
    "    quantity = ex_row['quantity'][i]\n",
    "    unit = ex_row['measurementUnit'][i]\n",
    "    food = ex_row['foodItem'][i]\n",
    "\n",
    "    if quantity > 0:\n",
    "        print(quantity, unit, food)\n",
    "    else:\n",
    "        print(food)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i, units in enumerate(relevant_cols_df['measurementUnit']):\n",
    "    for u in units:\n",
    "        if u == '<unit>':\n",
    "            idxs.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>healthLabels</th>\n",
       "      <th>recipeName</th>\n",
       "      <th>foodItem</th>\n",
       "      <th>quantity</th>\n",
       "      <th>measurementUnit</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sauteed Green Beans</td>\n",
       "      <td>[green beans, olive oil, green-bean]</td>\n",
       "      <td>[1.5, 1.0, 1.0]</td>\n",
       "      <td>[pound, tablespoon, &lt;unit&gt;]</td>\n",
       "      <td>15.008955</td>\n",
       "      <td>29.287441</td>\n",
       "      <td>12.551761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Frenched Green Beans</td>\n",
       "      <td>[green beans, salt, black pepper, Sherry vineg...</td>\n",
       "      <td>[2.0, 0.75, 0.25, 2.0, 1.5, 1.0]</td>\n",
       "      <td>[pound, teaspoon, teaspoon, teaspoon, tablespo...</td>\n",
       "      <td>22.281541</td>\n",
       "      <td>39.256213</td>\n",
       "      <td>16.777531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Lemon Green Beans recipes</td>\n",
       "      <td>[lemon, salt, green beans, olive oil]</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.5]</td>\n",
       "      <td>[&lt;unit&gt;, None, pound, tablespoon]</td>\n",
       "      <td>21.499903</td>\n",
       "      <td>24.845194</td>\n",
       "      <td>9.224740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Balanced</td>\n",
       "      <td>Southern Green Beans</td>\n",
       "      <td>[green beans, bacon, onion, red wine vinegar, ...</td>\n",
       "      <td>[1.25, 2.0, 1.0, 2.0, 2.0]</td>\n",
       "      <td>[pound, slice, &lt;unit&gt;, tablespoon, teaspoon]</td>\n",
       "      <td>22.835379</td>\n",
       "      <td>38.038953</td>\n",
       "      <td>19.103845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Crispy Green Beans</td>\n",
       "      <td>[flour, egg, buttermilk, panko breadcrumbs, ca...</td>\n",
       "      <td>[0.6666666666666666, 1.0, 0.6666666666666666, ...</td>\n",
       "      <td>[cup, &lt;unit&gt;, cup, cup, tablespoon, None, None...</td>\n",
       "      <td>95.584178</td>\n",
       "      <td>170.094762</td>\n",
       "      <td>39.403952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13265</th>\n",
       "      <td>Pescatarian</td>\n",
       "      <td>Tuna melt pizza baguettes</td>\n",
       "      <td>[baguette, red pepper, green pepper, sweetcorn...</td>\n",
       "      <td>[2.0, 1.0, 1.0, 198.0, 225.0, 100.0, 1.0]</td>\n",
       "      <td>[&lt;unit&gt;, &lt;unit&gt;, &lt;unit&gt;, gram, gram, gram, tab...</td>\n",
       "      <td>52.192541</td>\n",
       "      <td>343.546831</td>\n",
       "      <td>151.500834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13267</th>\n",
       "      <td>Pescatarian</td>\n",
       "      <td>Red Pepper Farro With Tuna</td>\n",
       "      <td>[olive oil, red bell pepper, farro, Salt, Pepp...</td>\n",
       "      <td>[2.0, 1.0, 1.0, 0.0, 0.0, 10.0]</td>\n",
       "      <td>[tablespoon, &lt;unit&gt;, cup, None, None, ounce]</td>\n",
       "      <td>34.505177</td>\n",
       "      <td>110.728103</td>\n",
       "      <td>82.228005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13269</th>\n",
       "      <td>Pescatarian</td>\n",
       "      <td>Savory Tuna Sandwich</td>\n",
       "      <td>[light tuna, black olives, capers, white vineg...</td>\n",
       "      <td>[16.0, 6.0, 2.0, 0.25, 1.0, 8.0, 2.0, 4.0]</td>\n",
       "      <td>[ounce, &lt;unit&gt;, teaspoon, cup, tablespoon, sli...</td>\n",
       "      <td>30.103793</td>\n",
       "      <td>164.929173</td>\n",
       "      <td>127.987906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13270</th>\n",
       "      <td>Pescatarian</td>\n",
       "      <td>Tiny Tuna Melts recipes</td>\n",
       "      <td>[tuna, mayonnaise, lemon juice, coarse salt, g...</td>\n",
       "      <td>[3.0, 1.0, 2.0, 0.0, 0.0, 16.0, 0.25]</td>\n",
       "      <td>[ounce, tablespoon, teaspoon, None, None, &lt;uni...</td>\n",
       "      <td>31.519834</td>\n",
       "      <td>24.483712</td>\n",
       "      <td>25.609022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13271</th>\n",
       "      <td>Pescatarian</td>\n",
       "      <td>Mexi-Cali Tuna Salad</td>\n",
       "      <td>[light tuna in water, canned black beans, sals...</td>\n",
       "      <td>[1.0, 0.75, 0.5, 0.5]</td>\n",
       "      <td>[can, cup, cup, &lt;unit&gt;]</td>\n",
       "      <td>18.539650</td>\n",
       "      <td>25.437450</td>\n",
       "      <td>75.942400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8722 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      healthLabels                  recipeName  \\\n",
       "1            Vegan         Sauteed Green Beans   \n",
       "10           Vegan        Frenched Green Beans   \n",
       "28           Vegan   Lemon Green Beans recipes   \n",
       "29        Balanced        Southern Green Beans   \n",
       "32      Vegetarian          Crispy Green Beans   \n",
       "...            ...                         ...   \n",
       "13265  Pescatarian   Tuna melt pizza baguettes   \n",
       "13267  Pescatarian  Red Pepper Farro With Tuna   \n",
       "13269  Pescatarian        Savory Tuna Sandwich   \n",
       "13270  Pescatarian     Tiny Tuna Melts recipes   \n",
       "13271  Pescatarian        Mexi-Cali Tuna Salad   \n",
       "\n",
       "                                                foodItem  \\\n",
       "1                   [green beans, olive oil, green-bean]   \n",
       "10     [green beans, salt, black pepper, Sherry vineg...   \n",
       "28                 [lemon, salt, green beans, olive oil]   \n",
       "29     [green beans, bacon, onion, red wine vinegar, ...   \n",
       "32     [flour, egg, buttermilk, panko breadcrumbs, ca...   \n",
       "...                                                  ...   \n",
       "13265  [baguette, red pepper, green pepper, sweetcorn...   \n",
       "13267  [olive oil, red bell pepper, farro, Salt, Pepp...   \n",
       "13269  [light tuna, black olives, capers, white vineg...   \n",
       "13270  [tuna, mayonnaise, lemon juice, coarse salt, g...   \n",
       "13271  [light tuna in water, canned black beans, sals...   \n",
       "\n",
       "                                                quantity  \\\n",
       "1                                        [1.5, 1.0, 1.0]   \n",
       "10                      [2.0, 0.75, 0.25, 2.0, 1.5, 1.0]   \n",
       "28                                  [1.0, 0.0, 1.0, 1.5]   \n",
       "29                            [1.25, 2.0, 1.0, 2.0, 2.0]   \n",
       "32     [0.6666666666666666, 1.0, 0.6666666666666666, ...   \n",
       "...                                                  ...   \n",
       "13265          [2.0, 1.0, 1.0, 198.0, 225.0, 100.0, 1.0]   \n",
       "13267                    [2.0, 1.0, 1.0, 0.0, 0.0, 10.0]   \n",
       "13269         [16.0, 6.0, 2.0, 0.25, 1.0, 8.0, 2.0, 4.0]   \n",
       "13270              [3.0, 1.0, 2.0, 0.0, 0.0, 16.0, 0.25]   \n",
       "13271                              [1.0, 0.75, 0.5, 0.5]   \n",
       "\n",
       "                                         measurementUnit        fat  \\\n",
       "1                            [pound, tablespoon, <unit>]  15.008955   \n",
       "10     [pound, teaspoon, teaspoon, teaspoon, tablespo...  22.281541   \n",
       "28                     [<unit>, None, pound, tablespoon]  21.499903   \n",
       "29          [pound, slice, <unit>, tablespoon, teaspoon]  22.835379   \n",
       "32     [cup, <unit>, cup, cup, tablespoon, None, None...  95.584178   \n",
       "...                                                  ...        ...   \n",
       "13265  [<unit>, <unit>, <unit>, gram, gram, gram, tab...  52.192541   \n",
       "13267       [tablespoon, <unit>, cup, None, None, ounce]  34.505177   \n",
       "13269  [ounce, <unit>, teaspoon, cup, tablespoon, sli...  30.103793   \n",
       "13270  [ounce, tablespoon, teaspoon, None, None, <uni...  31.519834   \n",
       "13271                            [can, cup, cup, <unit>]  18.539650   \n",
       "\n",
       "            carbs     protein  \n",
       "1       29.287441   12.551761  \n",
       "10      39.256213   16.777531  \n",
       "28      24.845194    9.224740  \n",
       "29      38.038953   19.103845  \n",
       "32     170.094762   39.403952  \n",
       "...           ...         ...  \n",
       "13265  343.546831  151.500834  \n",
       "13267  110.728103   82.228005  \n",
       "13269  164.929173  127.987906  \n",
       "13270   24.483712   25.609022  \n",
       "13271   25.437450   75.942400  \n",
       "\n",
       "[8722 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_cols_df.iloc[idxs,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['2 part-baked baguette', '1 red pepper, diced', '1 green pepper, diced', '198g can sweetcorn, drained', '225g jar tuna', '100g cheddar, grated', '1 tbsp tomato purée']\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ingredientLines'][13265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unit>', '<unit>', '<unit>', 'gram', 'gram', 'gram', 'tablespoon']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_cols_df.iloc[13265]['measurementUnit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These <unit> values are essentially missing data, even though it is in the ingredientLines column. We could cross reference to fill them in, but since we are going to concatenate all these foodItem, quantity, and measurementUnit columns to get the ingredientLines anyway, let's just use it directly. Then, to allow users to adjust the quantity, we will have to go through the ingredients and pull out the quantity instead, and put it in it's own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredientLines = df['ingredientLines'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>healthLabels</th>\n",
       "      <th>recipeName</th>\n",
       "      <th>ingredientLines</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Green Beans</td>\n",
       "      <td>[1 pound green beans, trimmed, 1 tablespoon bu...</td>\n",
       "      <td>12.559853</td>\n",
       "      <td>19.920021</td>\n",
       "      <td>8.567392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sauteed Green Beans</td>\n",
       "      <td>[1 1/2 lb green beans, stem ends trimmed, 1 ta...</td>\n",
       "      <td>15.008955</td>\n",
       "      <td>29.287441</td>\n",
       "      <td>12.551761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Caramelized Green Beans</td>\n",
       "      <td>[1 stick (8 tbsp.) unsalted, cultured butter, ...</td>\n",
       "      <td>93.626455</td>\n",
       "      <td>29.120751</td>\n",
       "      <td>13.416711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sautéed Fresh Green Beans</td>\n",
       "      <td>[2 teaspoons walnut oil, 1 pound green beans, ...</td>\n",
       "      <td>9.997903</td>\n",
       "      <td>19.368394</td>\n",
       "      <td>8.300740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Fancy Green Beans</td>\n",
       "      <td>[1 pound green beans, trimmed, 2 teaspoons veg...</td>\n",
       "      <td>11.359097</td>\n",
       "      <td>20.608817</td>\n",
       "      <td>9.509045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  healthLabels                 recipeName  \\\n",
       "0   Vegetarian                Green Beans   \n",
       "1        Vegan        Sauteed Green Beans   \n",
       "2   Vegetarian    Caramelized Green Beans   \n",
       "3        Vegan  Sautéed Fresh Green Beans   \n",
       "4        Vegan          Fancy Green Beans   \n",
       "\n",
       "                                     ingredientLines        fat      carbs  \\\n",
       "0  [1 pound green beans, trimmed, 1 tablespoon bu...  12.559853  19.920021   \n",
       "1  [1 1/2 lb green beans, stem ends trimmed, 1 ta...  15.008955  29.287441   \n",
       "2  [1 stick (8 tbsp.) unsalted, cultured butter, ...  93.626455  29.120751   \n",
       "3  [2 teaspoons walnut oil, 1 pound green beans, ...   9.997903  19.368394   \n",
       "4  [1 pound green beans, trimmed, 2 teaspoons veg...  11.359097  20.608817   \n",
       "\n",
       "     protein  \n",
       "0   8.567392  \n",
       "1  12.551761  \n",
       "2  13.416711  \n",
       "3   8.300740  \n",
       "4   9.509045  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_df = pd.concat([priority_health_labels, recipe_name, ingredientLines, macros_df], axis=1)\n",
    "relevant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to join/concatenate the list of ingredients in the ingredientLines column with a comma, \n",
    "#so let's get rid of the existing commas in the individual ingredients now\n",
    "def comma_to_bracket(ingredient_list):\n",
    "    \"\"\"\n",
    "    Input: ingredient_list (str): a list of strings, like ingredients of a recipe.\n",
    "    Output: recipe (str): commas in individual elements from input string are removed, then they are all joined together with a comma, so commas seperate each ingredient now.\n",
    "    \"\"\"\n",
    "    processed_ingredients = []\n",
    "    for ingredient in ingredient_list:\n",
    "        parts = ingredient.split(',', 1)  # Split at the first comma\n",
    "        if len(parts) > 1:  # Check if there is a comma\n",
    "            # Check if the part after the comma is already in brackets\n",
    "            if '(' not in parts[1] and ')' not in parts[1]:\n",
    "                parts[1] = f'({parts[1].strip()})'  # Put it in brackets\n",
    "        processed_ingredients.append(' '.join(parts))\n",
    "\n",
    "    # Join the processed strings with a comma and space now that we removed the commas in the individual strings\n",
    "    recipe = ', '.join(processed_ingredients)\n",
    "\n",
    "    return recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df['ingredientLines'] = relevant_df['ingredientLines'].apply(comma_to_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>healthLabels</th>\n",
       "      <th>recipeName</th>\n",
       "      <th>ingredientLines</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Green Beans</td>\n",
       "      <td>1 pound green beans (trimmed), 1 tablespoon bu...</td>\n",
       "      <td>12.559853</td>\n",
       "      <td>19.920021</td>\n",
       "      <td>8.567392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan</td>\n",
       "      <td>Sauteed Green Beans</td>\n",
       "      <td>1 1/2 lb green beans (stem ends trimmed), 1 ta...</td>\n",
       "      <td>15.008955</td>\n",
       "      <td>29.287441</td>\n",
       "      <td>12.551761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>Caramelized Green Beans</td>\n",
       "      <td>1 stick (8 tbsp.) unsalted (cultured butter), ...</td>\n",
       "      <td>93.626455</td>\n",
       "      <td>29.120751</td>\n",
       "      <td>13.416711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  healthLabels               recipeName  \\\n",
       "0   Vegetarian              Green Beans   \n",
       "1        Vegan      Sauteed Green Beans   \n",
       "2   Vegetarian  Caramelized Green Beans   \n",
       "\n",
       "                                     ingredientLines        fat      carbs  \\\n",
       "0  1 pound green beans (trimmed), 1 tablespoon bu...  12.559853  19.920021   \n",
       "1  1 1/2 lb green beans (stem ends trimmed), 1 ta...  15.008955  29.287441   \n",
       "2  1 stick (8 tbsp.) unsalted (cultured butter), ...  93.626455  29.120751   \n",
       "\n",
       "     protein  \n",
       "0   8.567392  \n",
       "1  12.551761  \n",
       "2  13.416711  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a model to predict the macros. But first we need to preprocess the input by concatenating the strings into just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch, random, datasets\n",
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = relevant_df['healthLabels'] + ' ' + relevant_df['recipeName'] + ' ' + relevant_df['ingredientLines']\n",
    "X = X.rename('fullRecipeInput')\n",
    "y = relevant_df[['fat', 'carbs', 'protein']]\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.concat([X, y], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['fullRecipeInput', 'fat', 'carbs', 'protein'],\n",
       "        num_rows: 10617\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['fullRecipeInput', 'fat', 'carbs', 'protein'],\n",
       "        num_rows: 2655\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.tolist(), y['fat'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "# Encode the text\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeTorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        label = {k: torch.tensor(v[idx], dtype=torch.float32) for k, v in self.labels.items()}\n",
    "        item.update(label)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = MakeTorchData(train_encodings, y_train)\n",
    "valid_dataset = MakeTorchData(valid_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "# Modify the number of labels to 3 (for the 3 regression targets)\n",
    "config.num_labels = 3\n",
    "\n",
    "# Instantiate the model with modified configuration\n",
    "model = BertForSequenceClassification(config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306121449ec444dd8e12d7cc1e018788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "7450",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7450",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 40\u001b[0m\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     43\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\transformers\\trainer.py:1836\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1836\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\accelerate\\data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m, in \u001b[0;36mMakeTorchData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      7\u001b[0m     item \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(v[idx]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m----> 8\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mupdate(label)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      7\u001b[0m     item \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(v[idx]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m----> 8\u001b[0m     label \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mupdate(label)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3807\u001b[0m     ):\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 7450"
     ]
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            loss += self.weights[i] * torch.mean((predictions[i] - targets[:, i])**2)\n",
    "        return loss\n",
    "\n",
    "loss_fn = CustomLoss(weights=[1.0, 1.0, 1.0])\n",
    "\n",
    "# Define TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=20,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='rmse',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=loss_fn,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"smape\": smape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a20a8fdbd844d3b712a46d40456a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "7450",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7450",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 26\u001b[0m\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,                         \n\u001b[0;32m     19\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,                  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     compute_metrics \u001b[38;5;241m=\u001b[39m compute_metrics_for_regression,     \n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Call the summary\u001b[39;00m\n\u001b[0;32m     29\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\transformers\\trainer.py:1836\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1836\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\accelerate\\data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m, in \u001b[0;36mMakeTorchData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      7\u001b[0m     item \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(v[idx]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m----> 8\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mupdate(label)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      7\u001b[0m     item \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(v[idx]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m----> 8\u001b[0m     label \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mupdate(label)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\edamam\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3807\u001b[0m     ):\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 7450"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = 1,     \n",
    "    per_device_train_batch_size = 32,   \n",
    "    per_device_eval_batch_size = 20,   \n",
    "    weight_decay = 0.01,               \n",
    "    learning_rate = 2e-5,\n",
    "    logging_dir = './logs',            \n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'rmse',    \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    ") \n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_dataset,         \n",
    "    eval_dataset = valid_dataset,          \n",
    "    compute_metrics = compute_metrics_for_regression,     \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputRegressionModel(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MultiOutputRegressionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, num_outputs)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        x = torch.relu(self.fc1(pooled_output))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 3  # Number of output dimensions\n",
    "model = MultiOutputRegressionModel(num_outputs=num_outputs)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edamam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
