


import pandas as pd
import ast


df = pd.read_csv('example_recipes.csv')


df.columns


ingredients_col = df['ingredients'].apply(ast.literal_eval)


ingredients_col[1][0]


dct = dict()
dct.keys()


#ingredients_col[1][0]['foodCategory']
dct[ingredients_col[1][0]['foodCategory']] = ingredients_col[1][0]['quantity']


dct = dict()
for i, row in ingredients_col.items():
    for j in range(len(row)):
        key = row[j]['foodCategory']
        value = row[j]['quantity']

        if key in dct.keys():
            dct[key] += value
        else:
            dct[key] = value


'text' in ingredients_col[0][0]


dct.keys()


ex_s = ''
ex_s_lst = ast.literal_eval(df.iloc[0]['ingredientLines'])
for s in ex_s_lst:
    ex_s += s + ', '

ex_s


', '.join(ex_s_lst)











priority_order = ['Vegan', 'Vegetarian', 'Pescatarian', 'Paleo', 'Red-Meat-Free', 'Mediterranean']


health_labels = df['healthLabels'].apply(ast.literal_eval)
health_labels


def replace_with_priority(labels):
    for label in priority_order:
        if label in labels:
            return label
    return 'Balanced'  # Handle case where no label matches priority_order, in which case the diet is balanced

# Apply function to the multilabels series
diet_type = health_labels.apply(replace_with_priority)


diet_type.value_counts()





recipe_name = df['label']





ingredients_lst = df['ingredientLines'].apply(ast.literal_eval)
ingredients_lst = ingredients_lst.apply(lambda x: ', '.join(x))
ingredients_lst





df2 = pd.concat([diet_type, recipe_name, ingredients_lst], axis = 1)
column_names = {'healthLabels': 'dietType', 'label': 'recipeName', 'ingredientLines': 'ingredientsList'}
df2 = df2.rename(columns=column_names)
df2.head()








from gretel_client import configure_session
from gretel_client.helpers import poll
from gretel_client.projects import create_or_get_unique_project, get_project
from gretel_client.projects.models import read_model_config, Model


LLM = "gretelai/mpt-7b"  # @param {type:"string"}
GRETEL_PROJECT = 'edamam'  # @param {type:"string"}
TEXT_COLUMN = "text"# @param {type:"string"}
LABEL_COLUMN = "ingredientsList" # @param {type:"string"}
LABEL_AND_TEXT_COLUMN = 'recipeTypeName'


config = read_model_config("synthetics/natural-language")
config['models'][0]['gpt_x']['pretrained_model'] = LLM
config['models'][0]['gpt_x']['column_name'] = LABEL_AND_TEXT_COLUMN
config


configure_session(api_key="prompt", cache="yes", endpoint="https://api.gretel.cloud", validate=True, clear=True)


project = create_or_get_unique_project(name=GRETEL_PROJECT)


df3 = pd.concat([df2['dietType'] + ' ' + df2['recipeName'], df2['ingredientsList']], axis = 1)
df3 = df3.rename(columns={0: 'recipeTypeName'})
df3.head(3)


model = project.create_model_obj(model_config=config, data_source=df3)
print(f"Follow along with training in the console: {project.get_console_url()}")
model.name = f"{GRETEL_PROJECT}-{LLM}"
model.submit_cloud()

poll(model)


PROMPT_LABEL = "Vegan Carrot Juice"  # @param {type:"string"}
NUM_RECORDS = 5  # @param {type:"number"}


def create_prompt_df(prompt_label: str, num_records: int = 25) -> pd.DataFrame:
    """
    Create a prompt DataFrame with the given number of rows, each containing a prompt.

    Args:
        prompt_label (str): The class label to use in the prompt.
        num_records (int): The number of records to generate in the prompt DataFrame.
            The generated synthetic data will have the same number of records.

    Returns:
        pd.DataFrame: A DataFrame with the given number of rows, each containing a class
            label prompt.
    """
    # Note: the column name in this dataframe doesn't matter, as it may only contain a single
    # column anyway.
    # The column name in the generated synthetic data will be taken from the training dataset
    # instead.
    return pd.DataFrame([prompt_label] * num_records, columns=["prompt"])


print("Text completion prompts with class labels")
prompt_df = create_prompt_df(PROMPT_LABEL, num_records=NUM_RECORDS)
prompt_df


response_handler = model.create_record_handler_obj(
        params={"maximum_text_length": 100, "temperature": 0.7},
        data_source=prompt_df
    )


response_handler.submit_cloud()
poll(response_handler)


pd.read_csv(response_handler.get_artifact_link("data"), compression='gzip')








import transformers
import torch
print("Transformers version:", transformers.__version__)
print("Torch version:", torch.__version__)

from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import T5ForConditionalGeneration, T5Tokenizer
from sklearn.model_selection import train_test_split


print(torch.cuda.is_available())


if torch.cuda.is_available():
    # Set the device to CUDA
    device = torch.device('cuda')
    print(f'Using GPU: {torch.cuda.get_device_name()}')
else:
    # If CUDA is not available, fall back to CPU
    device = torch.device('cpu')
    print('GPU is not available. Using CPU.')


#tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = T5ForConditionalGeneration.from_pretrained('t5-small')
tokenizer = T5Tokenizer.from_pretrained('t5-small')


inputs = df3['recipeTypeName'].tolist()
outputs = df3['ingredientsList'].tolist()


train_encodings = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)
train_labels = tokenizer(outputs, return_tensors='pt', padding=True, truncation=True)


# Define training parameters
batch_size = 4
learning_rate = 1e-4
num_epochs = 1


optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()


train_labels


for epoch in range(num_epochs):
    for i in range(0, len(train_encodings['input_ids']), batch_size):
        inputs = train_encodings['input_ids'][i:i+batch_size]
        labels = train_labels['input_ids'][i:i+batch_size]
        
        optimizer.zero_grad()
        outputs = model(input_ids=inputs, labels=labels)
        loss = criterion(outputs.logits.view(-1, outputs.logits.shape[-1]), labels.view(-1))
        loss.backward()
        optimizer.step()
    
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')





import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))
from datasets import load_dataset
import numpy as np
import matplotlib.pyplot as plt
import transformers
import datasets
from transformers import AutoTokenizer, TFT5ForConditionalGeneration
import datetime
import os
%load_ext tensorboard

tf_version = tf.__version__
print("Tensorflow: ", tf_version)
print("Transformers: ", transformers.__version__)
print("Datasets: ", datasets.__version__)


tf_version_split = tf_version.split('.')
assert int(tf_version_split[0])==2 and int(tf_version_split[-2])>=3, f"Tensorflow version should be '2.3+,x', given {tf_version}"


!mkdir data

data_dir = "./data"
log_dir = f"{data_dir}/experiments/t5/logs"
save_path = f"{data_dir}/experiments/t5/models"
cache_path_train = f"{data_dir}/cache/t5.train"
cache_path_test = f"{data_dir}/cache/t5.test"


class SnapthatT5(TFT5ForConditionalGeneration):
    def __init__(self, *args, log_dir=None, cache_dir= None, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_tracker= tf.keras.metrics.Mean(name='loss') 
    
    @tf.function
    def train_step(self, data):
        x = data
        y = x["labels"]
        y = tf.reshape(y, [-1, 1])
        with tf.GradientTape() as tape:
            outputs = self(x, training=True)
            loss = outputs[0]
            logits = outputs[1]
            loss = tf.reduce_mean(loss)
            
            grads = tape.gradient(loss, self.trainable_variables)
            
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        lr = self.optimizer._decayed_lr(tf.float32)
        
        self.loss_tracker.update_state(loss)        
        self.compiled_metrics.update_state(y, logits)
        metrics = {m.name: m.result() for m in self.metrics}
        metrics.update({'lr': lr})
        
        return metrics

    def test_step(self, data):
        x = data
        y = x["labels"]
        y = tf.reshape(y, [-1, 1])
        output = self(x, training=False)
        loss = output[0]
        loss = tf.reduce_mean(loss)
        logits = output[1]
        
        self.loss_tracker.update_state(loss)
        self.compiled_metrics.update_state(y, logits)
        return {m.name: m.result() for m in self.metrics}
        


tokenizer = AutoTokenizer.from_pretrained("t5-base", model_max_length=1024)


inputs = df3['recipeTypeName'].tolist()
outputs = df3['ingredientsList'].tolist()


encoder_inputs = tokenizer(inputs, 
                           truncation=True, 
                           return_tensors='tf', 
                           max_length=1024,
                           pad_to_max_length=True)

decoder_inputs = tokenizer(outputs, 
                           truncation=True, 
                           return_tensors='tf', 
                           max_length=1024,
                           pad_to_max_length=True)


input_ids = encoder_inputs['input_ids'][0]
input_attention = encoder_inputs['attention_mask'][0]
target_ids = decoder_inputs['input_ids'][0]
target_attention = decoder_inputs['attention_mask'][0]


outputs = {'input_ids':input_ids, 'attention_mask': input_attention, 
           'labels':target_ids, 'decoder_attention_mask':target_attention}


train_ds = outputs
train_ds


ex = next(iter(train_ds['input_ids']))
print("Example data from the mapped dataset:\n", ex)


def to_tf_dataset(dataset):  
  columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']
  dataset.set_format(type='tensorflow', columns=columns)
  return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 
                'labels':tf.int32, 'decoder_attention_mask':tf.int32,  }
  return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]), 
                  'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None])}
  ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)
  return ds


to_tf_dataset(train_ds)



